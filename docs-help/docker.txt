#NOTE : the # contains comments the $ contains the actual command line with placeholders between <>

##################link to install docker-machine ###############################

https://docs.docker.com/v17.09/machine/install-machine/#install-machine-directly

################## link to install virtualbox ###################################

https://itsfoss.com/install-virtualbox-ubuntu/

########### unitest docker#######################################################

https://blog.phonito.io/unit-testing-docker-images/

############# minikube or kubernetes ############################################

https://kubernetes.io/docs/tasks/tools/install-minikube/

################## how to install docker ########################################

#install
$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu Bion stable"

Now that you have the repository set up, run the following commands to install Docker:
$ sudo apt-get update
$ sudo apt-get install docker-ce

$ sudo systemctl enable docker
$ sudo systemctl start docker

#test the instalation
$ docker run --rm -ti ubuntu:latest /bin/bash

#or you will have the same result
$ sudo dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375

to see underlining host
$ docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh

/ # cat /etc/os-release
PRETTY_NAME="Docker for Mac"

/ # ps | grep dockerd
 1096 root       0:16 /usr/local/bin/dockerd -H unix:///var/run/docker.sock
                          --config-file /run/config/docker/daemon.json
                          --swarm-default-advertise-addr=eth0
                          --userland-proxy-path /usr/bin/vpnkit-expose-port
                          --storage-driver overlay2

/ # exit

#docker daemon configuration is in /etc/docker/daemon.json


#adding env to the docker container is done using -e <VAR-NAME>=<VAR-VALUE>

#debug so to export a container into a tarball

$ docker export <containerid> -o <fileName>

#Alpine is based on musl standard library and not glibc and have sh instead of bash. However, you can also install glibc and bash in Alpine Linux if you really need it, and this is often done in the case of JVM containers.

#to longin you should install
$ sudo apt install gnome-keyring

#Create a basic local repository
$ git clone https://github.com/spkane/basic-registry --config core.autocrlf=input
$ cp config.yml.sample config.yml
$ cp registry.key.sample registry.key
$ cp registry.crt.sample registry.crt

$ cp config.yml.sample config.yml
#Create key using
$openssl req -x509 -nodes -sha256 -newkey rsa:4096 -keyout registry.key -out registry.crt -days 14 -subj '/CN=127.0.0.1'
#create the user and password
$ docker run --entrypoint htpasswd spkane/basic-registry:latest -Bbn ${<username>} ${<password>} > htpasswd

$ docker build -t my-registry .
$ docker run -d -p 5000:5000 --name registry my-registry
$ docker logs registry
$ docker login 127.0.0.1:5000
#to keep images it could be done using -v /tmp/registry-data:/var/lib/registry

#Keep images small
$ docker run -d -p 8080:8080 adejonge/helloworld

#to enter with ssh
$ docker run -it --privileged --pid=host debian nsenter -t 1  -m -u -n -i sh

#to see what is inside container
$ docker exec -it <containerId> /bin/sh

#to expose all ports use --network host especial for ephemeral ports

#Docker run command first call docker create and then docker start
#change the random host name to a specific hostname
$docker run --rm -ti --hostname=<hostNameFQD> ubuntu:latest /bin/bash

#by default the container dns is the host resolve.conf but you could override using --dns=<dnsValue> and --dns-search=<dnsSearchHost>#mac address could be override by --mad-address=<macAddress>


################## HEALTHCHECK in Dockerfile
$ HEALTHCHECK --interval=30s \
    --timeout=10s
    --retries=3
    --start-period=60s
    CMD curl -f http://localhost:3000/health || exit 1
# where:
# --interval define the wait time between health checks
# --timeout defines how long Docker should wait if the health check does not respond
# --start-period define the time that it takes to start the container so it could reply to the query
# the command should deliver: 1 if it sucess, 0 if it fails and timeout

#This could be set also into the stack.yml for the docker swarm

################# Mount and volumes ####################################

#to mount directory from host machine by example by default shared are rw but it could ro using ro
$ docker run --rm -ti -v <host_directory>:<mount-directory>:ro ubuntu:latest /bin/bash

#option to shared between containter is z for private is Z like : <host_directory>:<mount_directory>:z
#you could mount the root directory as readonly using --read-only=true
#to mount a temporary file like /tmp the following option should be use --tmpfs/tmp:rw,noexec,nodev,nosuid,size=256M

#to create a volume to be attached to the container
$ docker volume create <name-of-volume>
#to inspect a created volume
$ docker volume inspect <name-of-volume>
#to mount a volume to a container use --mount source=<name-of-volume>,target=<target-directory>
#to delete the volume which will lost any data writed into the volume
$ docker volume rm <name-of-volume>


############################## Quota ######################################

#for the quota you could use : --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s --memory <memory_value> and also -cpu-#--memory-swap <memory value> is for both phisical memory and swap
quota also --cpus=<value between 0.01 and number of cpu>

#use --cpuset=cpu_number to set afinity to a specific hardware cpu
#to dimically update quotas you could use
$ docker update --cpus="1.5" <containerID1> ... <containerIDn>

#The telltale sign of out of memory is a container exit code of 137 and kernel out-of-memory (OOM) messages in the dmesg output.
#Docker has features that allow you to tune and disable the Linux OOM killer by using the --oom-kill-disable and the --oom-score-adj arguments to docker run.

#Block IO
#To set this weight on a container, you need to pass the --blkio-weight to your docker run command with a valid value. You can also target a specific device using the --blkio-weight-device option.

#As with CPU shares, tuning the weights is hard to get right in practice, but we can make it vastly simpler by limiting the maximum number of bytes or operations per second that are available to a container via its cgroup. The following settings let us control that:

#--device-read-bps     Limit read rate (bytes per second) from a device
#--device-read-iops    Limit read rate (IO per second) from a device
#--device-write-bps    Limit write rate (bytes per second) to a device
#--device-write-iops   Limit write rate (IO per second) to a device

#set the docker soft limit to 50 open files andhard limit to 150 open files
$ sudo dockerd --default-ulimit nofile=50:150

#to create a container use
$ docker create -p <docker_port:host:port> <imageName>
#to see all containers use
$ docker ps -a
#to start a container use
$ docker start <containerId>
#To autorestart containers use --restart:on-failure, then whenever the container exits with a nonzero exit code or --restart:unless stopped if you want to autorestart unless you specific stopped it, other options :no, always
#the --restart:<type of failure>:<nr of retries>
#stop the container send SIGTERM if you add -t <timeout> it will send SIGKILL after timeout
#on docker kill you could add the --signal=<name of the signal>

#to clean-up all dockers in dev there is the command:
$ docker system prune

#to delete all of containers on docker host use:
$ docker rm $(docker ps -a -q)

#to delete all images on docker host use:
$ docker rmmi $(docker images -q)

#Docker ps and docker images have filter capability using --filter like:
$ docker rm $(docker ps -a -q --filter 'exited!=0')
#of for untagged containers
$ docker rmi $(docker images -q -f "dangling=true")

#In most installations, /var/lib/docker will be the default root directory used to store images and containers. If you need to change this, you can edit your Docker startup scripts to launch the daemon, with the --data-root argument pointing to a new storage location. To test this by hand, you could run something like this:
$ sudo dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 --data-root="/data/docker"

#The syntax for pulling them from the registry is very similar, but note the @ in the tag.
$ docker pull ubuntu@sha256:2f9a...82cf

#Docker containers don’t, by default, start anything in the background like a full virtual machine would. They’re a lot lighter weight than that and therefore don’t start an init system.

#by default docker redirect input and output to the caller of the docker

#the other equivalence of the docker exec is nsenter but this should be run on the docker server host
#first obtain the pid of the docker process
$ PID=$(docker inspect --format \{{.State.Pid\}} <containerId>)
#the run 
$ sudo nsenter --target $PID --mount --uts --ipc --net --pid
#or in one line
$ sudo nsenter --target $(docker inspect --format \{{.State.Pid\}} <containerId>) --mount --uts --ipc --net --pid /bin/sh
# wrapper from git is https://github.com/Pithikos/docker-enter


#log are stored on /var/lib/docker/containers/<container_id>/ where the <container_id> in json format
#logging rotation: The default settings do not currently enable log rotation. You’ll want to make sure you specify the --log-opt max-size and --log-opt max-file settings if running in production. 
#to change to other type of log : he supported option that currently is the simplest for running Docker at scale is the option to send your container logs to syslog directly from Docker. You can specify this on the Docker command line with the --log-driver=syslog option or set it as the default in the daemon.json file for all containers. Then restart the daemon.

#You can log directly to a remote syslog-compatible server from a single container by setting the log option syslog-address similar to this: --log-opt syslog-address=udp://192.168.42.42:123.

#API running on docker like statistics in pretty-print format
$ curl --unix-socket /var/run/docker.sock http://v1/containers/91c86ec7b33f/stats | head -1 | python3 json.tool

#health check git location  https://github.com/spkane/rocketchat-hubot-demo.git

#docker events to monitor events on container
$ docker events
#or the equivalent
$ curl --unix-socket /var/run/docker.sock http://v1/events

#cadivisor to have charts on 8080 port of the docker container which run cadvisor or rest on 8080/api/v1.3/containers/
$ docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor google/cadvisor:latest
#for fedora or RHEL you need also the volume :  --volume=/cgroup:/cgroup \.

#In Dockerfile the COPY will copy the host file into container the ADD will get from network or untar the tar
#on ADD you could said what type of security --chown=UID:GID
#WORKDIR change the context of layers is not equivalent with RUN cd because the last one is not persisted
#Put on the first part of the Dockerfile the part that is not change every time so those layers will not be rebuild each time.
#use .dockerignore which is like .gitignore to filter the files which will be put pattern to exclude files from the directory.
#docker image save and load should be use to create a tarball from an image and recreate the image from the tarball.

#Image name: <registry URL>/<User or Org>/<name>:<tag>
#<registry URL> the default is docker.io but thare are more like :
# https://cloud.google.com/container-registry
# https://aws.amazon.com/ecr/
# https://azure.microsoft.com/en-us/services/container-registry/
# https://access.redhat.com/container/
# https://jfrog.com/integration/artifactory-docker-registry

########################NETWORK ###############################################
# Bridge is provided by docker and is local. Simple network based on Linux bridgs allowing networking on a single host
# Macvlan is provided by docker and is local. Configures multiple layer 2 (MAC) addresses on a single physical host interface
# Overlay is provided by docker and is global. Multinode-capable container network based on Virtual Extensible LAN (VXLan)
# Weave Net is provided by Weaveworks and is global. Simple, resilient, multihost Docker networking.
# Contiv Network Plugin is provided by Cisco and is global . Open Source container networking.

#To list network on host use:
$ docker network ls

#To specify the network range (subnet) when creating a new network you should use like this:
$ docker network create --driver bridge --subnet "10.1.0.0/16" test-net

#Host network should never be used in production is the same network for the container as the host (like the same ip)
#you could ask the running container to run in the same network as another container specifing  --network container:<containerName>
#to expose port use -p <host port>:<container port>/<protocol like tcp/udp> the tcp is the default protocol

############################ DOCKER COMPOSE #####################################################
#volumes: declare which volume to be created at first run, this will be not recreated after first run
#volumes inside service declaration could be a volume declared or a directory from host as is mount using -v
#by default docker-compose up is using interactive mode so the terminal si blocked to run as daemon use -d
#docker compose prefixed all containers with the name of directory in which reside docker-compose.yml
#you could scale the application at start usingd --scale <serviceName>=<nr of instances>
#you could scale later on using docker-compose sclale <serviceName>=<nr of instances>
#build option in yml is the directory (context) where it should find the Dockerfile it could be specified by: context: <directory where is the dockerfile> and dockerfile: <the name of the dockerfile if is not Dockerfile>
#To push docker compose use:
$ docker-compose -f Dockerfile push

#useful links : https://docs.docker.com/compose/   https://docs.docker.com/compose/compose-file/


############################### Orchestrators #####################################################
############################### Docker Swarm ######################################################
#Only one manager node for dev, test and demo, three manager in small to medium size swarm and five managers for large to extra large swarms
#a worker communicate only with three neighbors
#init the swarm using
$ docker swarm init
#join to the swarm
$ docker swarm join --token <join-token> <IP address>:2377
#where <join-token> is a token generated by the swarm leader and <IP address> is the address of the leader
#to list the nodes into the swarm use
$ docker node ls
#then using inspect find the ip

#create the default machine for
$ docker-machine create --driver virtualbox default
#start the default machine
$ docker-machine start default

#create the swarm
# get IP of Swarm leader
$ export IP=$(docker-machine ip node-1)
# init the Swarm
$ docker-machine ssh node-1 docker swarm init --advertise-addr $IP
# Get the Swarm join-token
$ export JOIN_TOKEN=$(docker-machine ssh node-1 docker swarm join-token worker -q)\
#join the swarm
$ docker-machine ssh $NODE_NAME docker swarm join --token $JOIN_TOKEN $IP:2377
#to promote a leader use docker node promote <nameOfNode>

#script to create auto
alias dm="docker-machine"
for NODE in `seq 1 5`; do
  NODE_NAME=node-${NODE}
  dm rm --force $NODE_NAME
  dm create --driver virtualbox $NODE_NAME
done
alias dms="docker-machine ssh"
export IP=$(docker-machine ip node-1)
dms node-1 docker swarm init --advertise-addr $IP;
export JOIN_TOKEN=$(dms node-1 docker swarm join-token worker -q);
for NODE in `seq 2 5`; do
  NODE_NAME="node-${NODE}"
  dms $NODE_NAME docker swarm join --token $JOIN_TOKEN $IP:2377
done;
dms node-1 docker node promote node-2 node-3

#to deploy a stack configuration to the docker swarm use:
$ docker stack deploy -c <stack.yaml> <nameOfStack>
#to list the stacks use:
$ docker stack ls
#to list services use:
$ docker service ls

#in yml the update_config: is used for rolling update using parallelism mean batches and delay between batches failure_action: <rollback> is used to know what to do if it fail, monitor: <time> is used to define how long newly deployed task should be monitored for health as a decision point whether or not to continue with the next batch
#in yml the expose port is <hostPort>:<containerPort>
#all logs are agregated into the call docker service logs without containter id. if you want logs from a specific container  add the task id which could be obtained using docker service ps
#if a node is stopped and then it return to started he will not get back the old tasks but it will be ready for a new workload

#healthcheck: has test:[....] interval: timeout: retries: start_period:
#Docker swarm could not be used to have blue-green deploy strategy

#Secrets created from stdin:
$ echo "sample secret value" | docker secret create sample-secret - 
#Secrets created from file:
$ docker secret create other-secret <filePath>

#Secrets are used by services and usually they are asigned to a service at creation like
$ docker service create --name web --secret api-secret-key --publish 8000:8000 fundamentalsofdocker/whoami:latest
#The secret will be in running container at /run/secrets/<secret-key>
#it could be customized like :
$ docker service create --name web -p 8000:8000 --secret source=api-secret-key,target=/run/my-secrets/api-secret-key fundamentalsofdocker/whoami:latest

#to simulate secrets in dev environment:
$ docker container run -d --name whoami -p 8000:8000 -v $(pwd)/dev-secrets:/run/secrets fundamentalsofdocker/whoami:latest

#for legacy application which use yaml 
file=/app/bin/app.conf
demo_secret=`cat /run/secret/demo-secret`
sed -i "s/<<demo-secret-value>>/$demo_secret/g" "$file"

#To update secrets into a running service first remove it
$ docker service update --secret-rm db-password web
#then add the update secret
$ docker service update --secret-add source=db-password-v2, target=db-password web


################################## KUBERNETES ##############################################
#links
https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux
https://kubernetes.io/docs/tasks/tools/install-minikube/

#install kubectl
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

#or 
$ sudo apt-get update && sudo apt-get install -y apt-transport-https
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
$ sudo apt-get update
$ sudo apt-get install -y kubectl

#install minikube
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
$ chmod +x minukube
$ sudo install minikube /usr/local/bin/
#start the minikube
$ minikube start --vm-driver=<driver_name>
$ minikube status
$ kubectl version
$ kubectl config current-context
$ kubectl get-contexts
$ kubectl config use-context minikube
$ kubectl get nodes
$ kubectl cluster-info

#update bash completion
$ echo 'source <(kubectl completion bash)' >>~/.bashrc
$ kubectl completion bash > kubectl
$ sudo mv kubectl /etc/bash_completion.d/

#it is ok to have workers in hybrid OS : linux an Windows Server 2010 from kubernetes 1.10

#useful commands

https://kubernetes.io/docs/reference/kubectl/cheatsheet/

#MINIKUBE does not have LoadBallancer integrated so you should use an external one or emulate but is not working ...
$ minikube tunnel

#To describe a pod use :
$ kubectl describe pod/<name Of Pod>

#To see the persisted volume claim use
$ kubectl get pvc

#To enter into a pod use:
$ kubectl exec -it <NameOfPod> -- /bin/sh

#example:
$ kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
$ kubectl expose deployment hello-minikube --type=NodePort --port=8080
$ minikube service hello-minikube

##
$ kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080 deployment "hello-minikube" created
$ kubectl expose deployment hello-minikube --type=NodePort service "hello-minikube" exposed
$ kubectl service hello-minikube --url

#full scale deployment example
$ git clone https://github.com/bluewhalebook/docker-up-and-running-2nd-edition.git

$ http://192.168.99.110:30171/documents/docker-up-and-running-public/sample.pdf?page=1

#bubernetes and load balancer

https://blog.codonomics.com/2019/02/loadbalancer-support-with-minikube-for-k8s.html

https://metallb.universe.tf/installation/network-addons/

https://stackoverflow.com/questions/44110876/kubernetes-service-external-ip-pending

#ClusterIp is used for services that are not supposed to be seen outside of cluster in contrast with NodePort and LoadBalancer
#StatefulSet is used for db and persistence in contrast with ReplicaSet

#kubectl edit <service> could be used to modify running service for blue-green roll

#Secret created manually using base64 with kind Secret and type Opaque and data with secret could be retrieved back using kubectl get secrets/<secretName> -o yaml
#it should be created using kubectl command like
$ kubectl create secret generic pets-secret-prod --from-file=./username.txt --from-file=./password.txt
#to be used use in yaml the:
#volumeMounts:
# - name: secrets
#   mountPath: "/etc/secrets"
#   readOnly: true
#volumes:
# - name: secrets
#   secret:
#      secretName: pets-secret
#to be available in env use:
# env:
# - name: <PETS_USERNAME>
#   valueFrom:
#	secretKeyRef:
#		name:
#		key