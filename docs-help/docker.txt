# HAProxy for free LoadBalancer but I do not know if it works with docker
#links for docker gits:

https://github.com/fundamentalsofdocker/labs

#documentations:

https://docs.docker.com/engine/swarm/

https://docs.docker.com/compose/

https://stackoverflow.com/questions/42208442/maven-docker-cache-dependencies

https://docs.docker.com/engine/reference/commandline/push/



#NOTE : the # contains comments the $ contains the actual command line with placeholders between <>

##################link to install docker-machine ###############################

https://docs.docker.com/v17.09/machine/install-machine/#install-machine-directly

################## link to install virtualbox ###################################

https://itsfoss.com/install-virtualbox-ubuntu/

########### unitest docker#######################################################

https://blog.phonito.io/unit-testing-docker-images/

############# minikube or kubernetes ############################################

https://kubernetes.io/docs/tasks/tools/install-minikube/
################## docker info ##################################################
# CE use moby proiect
# cli use REST api to talk to the daemon
# docker-machine is part of the docker toolbox and is the old version no longer supported
# docker-machine use virtualbox which is a vm
# docker use feature from linux kernel called namespaces, some of them are: process, net, mount
# link is deprecated to link one container with another container by it's name
# the default network is bridge and if you don't create a network this will be use by docker
# if you link multiple containers to a use defined network the name of the container the hostname
# docker cli use REST API to talk to the daemon
# docker container ls  is the same as docker ls  but you should use management command not the aliases
# set DOCKER_HIDE_LEGACY_COMMANDS=true to hide lagacy commands but only for help
# an image could be taged multiple
# official repository are managed by docker.inc or their 3-rd parties that they trust
# official repository has info about security vulnerabilities
# comment is done with the # at the begining of the line and not inline comment
# docker plugin install ... this will install by example new drivers for net, see on docker hub the plugin section
# online repository: https://quay.io/

################## how to install docker ########################################

#install
$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu Bion stable"

Now that you have the repository set up, run the following commands to install Docker:
$ sudo apt-get update
$ sudo apt-get install docker-ce

$ sudo systemctl enable docker
$ sudo systemctl start docker

#test the instalation
$ docker run --rm -ti ubuntu:latest /bin/bash

#or you will have the same result
$ sudo dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375

to see underlining host
$ docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh

/ # cat /etc/os-release
PRETTY_NAME="Docker for Mac"

/ # ps | grep dockerd
 1096 root       0:16 /usr/local/bin/dockerd -H unix:///var/run/docker.sock
                          --config-file /run/config/docker/daemon.json
                          --swarm-default-advertise-addr=eth0
                          --userland-proxy-path /usr/bin/vpnkit-expose-port
                          --storage-driver overlay2

/ # exit

#docker daemon configuration is in /etc/docker/daemon.json
#add the user to run without sudo
$sudo groupadd docker
$sudo usermod -aG docker $USER

#docker toolbox and GUI has:
# Docker Machine, Docker Engine, Docker Compose, Docker GUI (Kitematic). Oracle VirtualBox, preconfigured Shell
# Docker machine is used to manage docker hosts
# on windows has docker notary cli to sign containers
# container does not need hypervisor as the VM need because container is just another process


#adding env to the docker container is done using -e <VAR-NAME>=<VAR-VALUE>

#debug so to export a container into a tarball

$ docker export <containerid> -o <fileName>

#Alpine is based on musl standard library and not glibc and have sh instead of bash. However, you can also install glibc and bash in Alpine Linux if you really need it, and this is often done in the case of JVM containers.

#to longin you should install
$ sudo apt install gnome-keyring

#Create a basic local repository
$ git clone https://github.com/spkane/basic-registry --config core.autocrlf=input
$ cp config.yml.sample config.yml
$ cp registry.key.sample registry.key
$ cp registry.crt.sample registry.crt

$ cp config.yml.sample config.yml
#Create key using
$openssl req -x509 -nodes -sha256 -newkey rsa:4096 -keyout registry.key -out registry.crt -days 14 -subj '/CN=127.0.0.1'
#create the user and password
$ docker run --entrypoint htpasswd spkane/basic-registry:latest -Bbn ${<username>} ${<password>} > htpasswd

$ docker build -t my-registry .
$ docker run -d -p 5000:5000 --name registry my-registry
$ docker logs registry
$ docker login 127.0.0.1:5000
#to keep images it could be done using -v /tmp/registry-data:/var/lib/registry

#Keep images small
$ docker run -d -p 8080:8080 adejonge/helloworld

#to enter with ssh
$ docker run -it --privileged --pid=host debian nsenter -t 1  -m -u -n -i sh

#to see what is inside container
$ docker exec -it <containerId> /bin/sh

#to expose all ports use --network host especial for ephemeral ports

#Docker run command first call docker create and then docker start
#change the random host name to a specific hostname
$docker run --rm -ti --hostname=<hostNameFQD> ubuntu:latest /bin/bash

#by default the container dns is the host resolve.conf but you could override using --dns=<dnsValue> and --dns-search=<dnsSearchHost>#mac address could be override by --mad-address=<macAddress>
#### docker cli #####
# use -it every time you run an interactive commands or -d in detach mode
# use CTRL-P CTRL-Q will detach from the container use attach to go back to container
# script to use to delete all containers (use carefully if you dont't want to remove a stoped container)
$ docker container rm $(docker container ls -aq)
# use --rm it will delete the container when it is exit

####DOCKERFILE
# FROM loads an image
# WORKDIR sets up a working directory
# EXPOSE identifies a port for app to listen on
# CMD runs the app and passes parameters
# ENV set up env variable
# ADD/COPY copy files from host system
# ENTRYPOINT target a and runs a specifica application


## MICROSERVICES
# install docker on ubuntu mate
$ apt-get install docker.io
# to validate the docker installation
$ docker run hello-world



################## HEALTHCHECK in Dockerfile
$ HEALTHCHECK --interval=30s \
    --timeout=10s
    --retries=3
    --start-period=60s
    CMD curl -f http://localhost:3000/health || exit 1
# where:
# --interval define the wait time between health checks
# --timeout defines how long Docker should wait if the health check does not respond
# --start-period define the time that it takes to start the container so it could reply to the query
# the command should deliver: 1 if it sucess, 0 if it fails and timeout

#This could be set also into the stack.yml for the docker swarm

################# Mount and volumes ####################################

#to mount directory from host machine by example by default shared are rw but it could ro using ro
$ docker run --rm -ti -v <host_directory>:<mount-directory>:ro ubuntu:latest /bin/bash

#option to shared between containter is z for private is Z like : <host_directory>:<mount_directory>:z
#you could mount the root directory as readonly using --read-only=true
#to mount a temporary file like /tmp the following option should be use --tmpfs/tmp:rw,noexec,nodev,nosuid,size=256M

#to create a volume to be attached to the container
$ docker volume create <name-of-volume>
#to inspect a created volume
$ docker volume inspect <name-of-volume>
#to mount a volume to a container use --mount source=<name-of-volume>,target=<target-directory>
#to delete the volume which will lost any data writed into the volume
$ docker volume rm <name-of-volume>

#Type of backends:
# Overlay and overlay2 is a union filesystem where multiple layers are mounted together so they appear as a single file system
# AuFs is the original one and is no longer recommended
# Btrfs is a copy on write filesystem, it is ok but does not support SELinux
# Device Mapper is on RedHat OS. By default is use loop-lvm mode which has zero configuration which is very slow, for production use direct-lvm
# VFS is the simplest and slowest. Is very slow to create new containers but runtime is native
# ZFS for production with increase cost due to license
# selecting storage is used by example :
$ dockerd --storage-driver=devicemapper


############################## Quota ######################################

#for the quota you could use : --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s --memory <memory_value> and also -cpu-#--memory-swap <memory value> is for both phisical memory and swap
quota also --cpus=<value between 0.01 and number of cpu>

#use --cpuset=cpu_number to set afinity to a specific hardware cpu
#to dimically update quotas you could use
$ docker update --cpus="1.5" <containerID1> ... <containerIDn>

#The telltale sign of out of memory is a container exit code of 137 and kernel out-of-memory (OOM) messages in the dmesg output.
#Docker has features that allow you to tune and disable the Linux OOM killer by using the --oom-kill-disable and the --oom-score-adj arguments to docker run.

#Block IO
#To set this weight on a container, you need to pass the --blkio-weight to your docker run command with a valid value. You can also target a specific device using the --blkio-weight-device option.

#As with CPU shares, tuning the weights is hard to get right in practice, but we can make it vastly simpler by limiting the maximum number of bytes or operations per second that are available to a container via its cgroup. The following settings let us control that:

#--device-read-bps     Limit read rate (bytes per second) from a device
#--device-read-iops    Limit read rate (IO per second) from a device
#--device-write-bps    Limit write rate (bytes per second) to a device
#--device-write-iops   Limit write rate (IO per second) to a device

#set the docker soft limit to 50 open files andhard limit to 150 open files
$ sudo dockerd --default-ulimit nofile=50:150

#to create a container use
$ docker create -p <docker_port:host:port> <imageName>
#to see all containers use
$ docker ps -a
#to start a container use
$ docker start <containerId>
#To autorestart containers use --restart:on-failure, then whenever the container exits with a nonzero exit code or --restart:unless stopped if you want to autorestart unless you specific stopped it, other options :no, always
#the --restart:<type of failure>:<nr of retries>
#stop the container send SIGTERM if you add -t <timeout> it will send SIGKILL after timeout
#on docker kill you could add the --signal=<name of the signal>

#to clean-up all dockers in dev there is the command:
$ docker system prune

#to delete all of containers on docker host use:
$ docker rm $(docker ps -a -q)

#to delete all images on docker host use:
$ docker rmmi $(docker images -q)

#Docker ps and docker images have filter capability using --filter like:
$ docker rm $(docker ps -a -q --filter 'exited!=0')
#of for untagged containers
$ docker rmi $(docker images -q -f "dangling=true")

#In most installations, /var/lib/docker will be the default root directory used to store images and containers. If you need to change this, you can edit your Docker startup scripts to launch the daemon, with the --data-root argument pointing to a new storage location. To test this by hand, you could run something like this:
$ sudo dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 --data-root="/data/docker"

#The syntax for pulling them from the registry is very similar, but note the @ in the tag.
$ docker pull ubuntu@sha256:2f9a...82cf

#Docker containers don’t, by default, start anything in the background like a full virtual machine would. They’re a lot lighter weight than that and therefore don’t start an init system.

#by default docker redirect input and output to the caller of the docker

#the other equivalence of the docker exec is nsenter but this should be run on the docker server host
#first obtain the pid of the docker process
$ PID=$(docker inspect --format \{{.State.Pid\}} <containerId>)
#the run 
$ sudo nsenter --target $PID --mount --uts --ipc --net --pid
#or in one line
$ sudo nsenter --target $(docker inspect --format \{{.State.Pid\}} <containerId>) --mount --uts --ipc --net --pid /bin/sh
# wrapper from git is https://github.com/Pithikos/docker-enter


#log are stored on /var/lib/docker/containers/<container_id>/ where the <container_id> in json format
#logging rotation: The default settings do not currently enable log rotation. You’ll want to make sure you specify the --log-opt max-size and --log-opt max-file settings if running in production. 
#to change to other type of log : he supported option that currently is the simplest for running Docker at scale is the option to send your container logs to syslog directly from Docker. You can specify this on the Docker command line with the --log-driver=syslog option or set it as the default in the daemon.json file for all containers. Then restart the daemon.

#You can log directly to a remote syslog-compatible server from a single container by setting the log option syslog-address similar to this: --log-opt syslog-address=udp://192.168.42.42:123.

#API running on docker like statistics in pretty-print format
$ curl --unix-socket /var/run/docker.sock http://v1/containers/91c86ec7b33f/stats | head -1 | python3 json.tool

#health check git location  https://github.com/spkane/rocketchat-hubot-demo.git

#docker events to monitor events on container
$ docker events
#or the equivalent
$ curl --unix-socket /var/run/docker.sock http://v1/events

#cadivisor to have charts on 8080 port of the docker container which run cadvisor or rest on 8080/api/v1.3/containers/
$ docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor google/cadvisor:latest
#for fedora or RHEL you need also the volume :  --volume=/cgroup:/cgroup \.

#In Dockerfile the COPY will copy the host file into container the ADD will get from network or untar the tar
#on ADD you could said what type of security --chown=UID:GID
#WORKDIR change the context of layers is not equivalent with RUN cd because the last one is not persisted
#Put on the first part of the Dockerfile the part that is not change every time so those layers will not be rebuild each time.
#use .dockerignore which is like .gitignore to filter the files which will be put pattern to exclude files from the directory.
#docker image save and load should be use to create a tarball from an image and recreate the image from the tarball.

#Image name: <registry URL>/<User or Org>/<name>:<tag>
#<registry URL> the default is docker.io but thare are more like :
# https://cloud.google.com/container-registry
# https://aws.amazon.com/ecr/
# https://azure.microsoft.com/en-us/services/container-registry/
# https://access.redhat.com/container/
# https://jfrog.com/integration/artifactory-docker-registry

# docker inspect could use --format to filter the result

########################NETWORK ###############################################
# Bridge is provided by docker and is local. Simple network based on Linux bridgs allowing networking on a single host
# Macvlan is provided by docker and is local. Configures multiple layer 2 (MAC) addresses on a single physical host interface. When migrating legacy system to a container-native one, this can be a really useful step.
# Overlay is provided by docker and is global. Multinode-capable container network based on Virtual Extensible LAN (VXLan) is used by docker swarm
# Weave Net is provided by Weaveworks and is global. Simple, resilient, multihost Docker networking.
# Contiv Network Plugin is provided by Cisco and is global . Open Source container networking.

#To list network on host use:
$ docker network ls

#To specify the network range (subnet) when creating a new network you should use like this:
$ docker network create --driver bridge --subnet "10.1.0.0/16" test-net

#Host network should never be used in production is the same network for the container as the host (like the same ip)
#you could ask the running container to run in the same network as another container specifing  --network container:<containerName>
#to expose port use -p <host port>:<container port>/<protocol like tcp/udp> the tcp is the default protocol
$ docker network inspect <network_id>

############################ DOCKER COMPOSE #####################################################

# githubs links: https://github.com/avescodes/docker-compose-in-depth

#volumes: declare which volume to be created at first run, this will be not recreated after first run
#volumes inside service declaration could be a volume declared or a directory from host as is mount using -v
#by default docker-compose up is using interactive mode so the terminal si blocked to run as daemon use -d
#docker compose prefixed all containers with the name of directory in which reside docker-compose.yml
#you could scale the application at start usingd --scale <serviceName>=<nr of instances>
#you could scale later on using docker-compose sclale <serviceName>=<nr of instances>
#build option in yml is the directory (context) where it should find the Dockerfile it could be specified by: context: <directory where is the dockerfile> and dockerfile: <the name of the dockerfile if is not Dockerfile>
#To push docker compose use:
$ docker-compose -f Dockerfile push

#useful links : https://docs.docker.com/compose/   https://docs.docker.com/compose/compose-file/
#link use <service>:<name> will deliver the property name to the container
# docker-compose rm #remove data from a containers
# docker-compose network default is bridge which link all containers
$ docker-compose exec <service_name> sh
# container_name is global even to docker host machine and could not scale using docker-compose scale
# links should be use in the same network it was working before in version 1
# when contianer is replaced the old connection between the remaining container and the old remain open but no data is transfer but a new connection will work !!
# use external_links to make an internal name to an external container which is connected to an external network, both should be share the network (external network should be defined in networks as external: true)
# use docker-compose port to find out which port is mapped to docker host
$ docker-compose port <service_name> <port_number>
# use external for external volume also
# see docs.docker.com/engine/extend/plugins/ to see the supported volume plugins
# docs.docker.com/engine/admin/logging/overview
# to get logs from the containers the default driver is json-file
$ docker-compose logs <container_id>
# paptertrailapp.com for free logging service to be used: driver is syslog and options is syslog-address: "udp://logs4.papertrailapp.com:33624"
# when you use external system docmer-compose logs is not available
########## docker-compose CLI ###############
# the dirrectory is used as prefix for the container names
# COMPOSE_NAME_PROJECT or --project-name when start an evironment instead of directory
# doenv from github is a good project to mangage development environments
# docker-composer events give events from the docker compose
# docker-compose config check the config file and reports errors if it has also if is correct it will print it
# orphan containers are containers that have been run using docker-compose run command
# in docker-compose.yml it is possible to run with ${}
# .env is read the by the docker-compose and replace the environments
# each service could have a key env_file: with a list of files.
# on evironment key if you specify variable without value it will be taken from the external environment
# configuration files are loaded left to right
# docker-compose.override.yml will override configuration from the docker-compose file but only if you don't specify file with -f


############################## DOCKER SECURITY ####################################################

#run the container in privileged mode so it you want to add device mount etc : add at container start --privileged=true
#The container could be run with non privileged user using -u <userId> otherwise is root
#all containers could be run with non priviledged user using --serns-remap in dockerd command
#if in priviledged container you mount swap using swapon <swap file> be sure you do swapoff before exit the container otherwise the container couln't be destroy
#if you forget that you could unmount from host using root like:
$ sudo swapoff /var/lib/docker/overlay/<containerId>/upper/<swap_file>
#if you want only some priviledged use --cap-add=<priviledge> and if you want to disable some priviledge you use --cap-drop=<priviledge>
#you could use  seccomp profile file to fine grain the capability of the container
#use -tlsverify to require certificates if you are in production

############################### DOCKER ARCHITECTURE #################################################

# dockerd - one per server, build container images, management : high level network, volumes, logging, statistics etc
# containerd - one per server, manages lifecycle execution, copy on write fileisystem an low level network
# docker-containerd-shim - one per container, handle IO and reports exit status
# runc - constructs the container and execution, statistics, reports events on lifecycle
#runtime continer could be Clear Containers (cc-runtime), runc or Railcar to select one use
$ docker run --runtime <runtime> -d <command>
#to see which container works with by example use :
$ sudo cc-runtime list
#gVisor is for high secure isolation and for for massive scalling, the runtime is runsc

############################### OTHER STUFF #######################################################

#If you use a process manager or initialization system on your servers, like upstart or systemd, it is usually very easy to direct all process output to STDOUT and STDERR and then have your process monitor capture them and send them to a remote logging host.
# progrim for loadbalancer
# infrastructure teraform





