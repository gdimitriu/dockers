# HAProxy for free LoadBalancer but I do not know if it works with docker
#links for docker gits:

https://github.com/fundamentalsofdocker/labs

#documentations:

https://docs.docker.com/engine/swarm/

https://docs.docker.com/compose/

https://stackoverflow.com/questions/42208442/maven-docker-cache-dependencies

https://docs.docker.com/engine/reference/commandline/push/



#NOTE : the # contains comments the $ contains the actual command line with placeholders between <>

##################link to install docker-machine ###############################

https://docs.docker.com/v17.09/machine/install-machine/#install-machine-directly

################## link to install virtualbox ###################################

https://itsfoss.com/install-virtualbox-ubuntu/

########### unitest docker#######################################################

https://blog.phonito.io/unit-testing-docker-images/

############# minikube or kubernetes ############################################

https://kubernetes.io/docs/tasks/tools/install-minikube/

################## how to install docker ########################################

#install
$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu Bion stable"

Now that you have the repository set up, run the following commands to install Docker:
$ sudo apt-get update
$ sudo apt-get install docker-ce

$ sudo systemctl enable docker
$ sudo systemctl start docker

#test the instalation
$ docker run --rm -ti ubuntu:latest /bin/bash

#or you will have the same result
$ sudo dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375

to see underlining host
$ docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh

/ # cat /etc/os-release
PRETTY_NAME="Docker for Mac"

/ # ps | grep dockerd
 1096 root       0:16 /usr/local/bin/dockerd -H unix:///var/run/docker.sock
                          --config-file /run/config/docker/daemon.json
                          --swarm-default-advertise-addr=eth0
                          --userland-proxy-path /usr/bin/vpnkit-expose-port
                          --storage-driver overlay2

/ # exit

#docker daemon configuration is in /etc/docker/daemon.json


#adding env to the docker container is done using -e <VAR-NAME>=<VAR-VALUE>

#debug so to export a container into a tarball

$ docker export <containerid> -o <fileName>

#Alpine is based on musl standard library and not glibc and have sh instead of bash. However, you can also install glibc and bash in Alpine Linux if you really need it, and this is often done in the case of JVM containers.

#to longin you should install
$ sudo apt install gnome-keyring

#Create a basic local repository
$ git clone https://github.com/spkane/basic-registry --config core.autocrlf=input
$ cp config.yml.sample config.yml
$ cp registry.key.sample registry.key
$ cp registry.crt.sample registry.crt

$ cp config.yml.sample config.yml
#Create key using
$openssl req -x509 -nodes -sha256 -newkey rsa:4096 -keyout registry.key -out registry.crt -days 14 -subj '/CN=127.0.0.1'
#create the user and password
$ docker run --entrypoint htpasswd spkane/basic-registry:latest -Bbn ${<username>} ${<password>} > htpasswd

$ docker build -t my-registry .
$ docker run -d -p 5000:5000 --name registry my-registry
$ docker logs registry
$ docker login 127.0.0.1:5000
#to keep images it could be done using -v /tmp/registry-data:/var/lib/registry

#Keep images small
$ docker run -d -p 8080:8080 adejonge/helloworld

#to enter with ssh
$ docker run -it --privileged --pid=host debian nsenter -t 1  -m -u -n -i sh

#to see what is inside container
$ docker exec -it <containerId> /bin/sh

#to expose all ports use --network host especial for ephemeral ports

#Docker run command first call docker create and then docker start
#change the random host name to a specific hostname
$docker run --rm -ti --hostname=<hostNameFQD> ubuntu:latest /bin/bash

#by default the container dns is the host resolve.conf but you could override using --dns=<dnsValue> and --dns-search=<dnsSearchHost>#mac address could be override by --mad-address=<macAddress>


################## HEALTHCHECK in Dockerfile
$ HEALTHCHECK --interval=30s \
    --timeout=10s
    --retries=3
    --start-period=60s
    CMD curl -f http://localhost:3000/health || exit 1
# where:
# --interval define the wait time between health checks
# --timeout defines how long Docker should wait if the health check does not respond
# --start-period define the time that it takes to start the container so it could reply to the query
# the command should deliver: 1 if it sucess, 0 if it fails and timeout

#This could be set also into the stack.yml for the docker swarm

################# Mount and volumes ####################################

#to mount directory from host machine by example by default shared are rw but it could ro using ro
$ docker run --rm -ti -v <host_directory>:<mount-directory>:ro ubuntu:latest /bin/bash

#option to shared between containter is z for private is Z like : <host_directory>:<mount_directory>:z
#you could mount the root directory as readonly using --read-only=true
#to mount a temporary file like /tmp the following option should be use --tmpfs/tmp:rw,noexec,nodev,nosuid,size=256M

#to create a volume to be attached to the container
$ docker volume create <name-of-volume>
#to inspect a created volume
$ docker volume inspect <name-of-volume>
#to mount a volume to a container use --mount source=<name-of-volume>,target=<target-directory>
#to delete the volume which will lost any data writed into the volume
$ docker volume rm <name-of-volume>

#Type of backends:
# Overlay and overlay2 is a union filesystem where multiple layers are mounted together so they appear as a single file system
# AuFs is the original one and is no longer recommended
# Btrfs is a copy on write filesystem, it is ok but does not support SELinux
# Device Mapper is on RedHat OS. By default is use loop-lvm mode which has zero configuration which is very slow, for production use direct-lvm
# VFS is the simplest and slowest. Is very slow to create new containers but runtime is native
# ZFS for production with increase cost due to license
# selecting storage is used by example :
$ dockerd --storage-driver=devicemapper


############################## Quota ######################################

#for the quota you could use : --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 120s --memory <memory_value> and also -cpu-#--memory-swap <memory value> is for both phisical memory and swap
quota also --cpus=<value between 0.01 and number of cpu>

#use --cpuset=cpu_number to set afinity to a specific hardware cpu
#to dimically update quotas you could use
$ docker update --cpus="1.5" <containerID1> ... <containerIDn>

#The telltale sign of out of memory is a container exit code of 137 and kernel out-of-memory (OOM) messages in the dmesg output.
#Docker has features that allow you to tune and disable the Linux OOM killer by using the --oom-kill-disable and the --oom-score-adj arguments to docker run.

#Block IO
#To set this weight on a container, you need to pass the --blkio-weight to your docker run command with a valid value. You can also target a specific device using the --blkio-weight-device option.

#As with CPU shares, tuning the weights is hard to get right in practice, but we can make it vastly simpler by limiting the maximum number of bytes or operations per second that are available to a container via its cgroup. The following settings let us control that:

#--device-read-bps     Limit read rate (bytes per second) from a device
#--device-read-iops    Limit read rate (IO per second) from a device
#--device-write-bps    Limit write rate (bytes per second) to a device
#--device-write-iops   Limit write rate (IO per second) to a device

#set the docker soft limit to 50 open files andhard limit to 150 open files
$ sudo dockerd --default-ulimit nofile=50:150

#to create a container use
$ docker create -p <docker_port:host:port> <imageName>
#to see all containers use
$ docker ps -a
#to start a container use
$ docker start <containerId>
#To autorestart containers use --restart:on-failure, then whenever the container exits with a nonzero exit code or --restart:unless stopped if you want to autorestart unless you specific stopped it, other options :no, always
#the --restart:<type of failure>:<nr of retries>
#stop the container send SIGTERM if you add -t <timeout> it will send SIGKILL after timeout
#on docker kill you could add the --signal=<name of the signal>

#to clean-up all dockers in dev there is the command:
$ docker system prune

#to delete all of containers on docker host use:
$ docker rm $(docker ps -a -q)

#to delete all images on docker host use:
$ docker rmmi $(docker images -q)

#Docker ps and docker images have filter capability using --filter like:
$ docker rm $(docker ps -a -q --filter 'exited!=0')
#of for untagged containers
$ docker rmi $(docker images -q -f "dangling=true")

#In most installations, /var/lib/docker will be the default root directory used to store images and containers. If you need to change this, you can edit your Docker startup scripts to launch the daemon, with the --data-root argument pointing to a new storage location. To test this by hand, you could run something like this:
$ sudo dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 --data-root="/data/docker"

#The syntax for pulling them from the registry is very similar, but note the @ in the tag.
$ docker pull ubuntu@sha256:2f9a...82cf

#Docker containers don’t, by default, start anything in the background like a full virtual machine would. They’re a lot lighter weight than that and therefore don’t start an init system.

#by default docker redirect input and output to the caller of the docker

#the other equivalence of the docker exec is nsenter but this should be run on the docker server host
#first obtain the pid of the docker process
$ PID=$(docker inspect --format \{{.State.Pid\}} <containerId>)
#the run 
$ sudo nsenter --target $PID --mount --uts --ipc --net --pid
#or in one line
$ sudo nsenter --target $(docker inspect --format \{{.State.Pid\}} <containerId>) --mount --uts --ipc --net --pid /bin/sh
# wrapper from git is https://github.com/Pithikos/docker-enter


#log are stored on /var/lib/docker/containers/<container_id>/ where the <container_id> in json format
#logging rotation: The default settings do not currently enable log rotation. You’ll want to make sure you specify the --log-opt max-size and --log-opt max-file settings if running in production. 
#to change to other type of log : he supported option that currently is the simplest for running Docker at scale is the option to send your container logs to syslog directly from Docker. You can specify this on the Docker command line with the --log-driver=syslog option or set it as the default in the daemon.json file for all containers. Then restart the daemon.

#You can log directly to a remote syslog-compatible server from a single container by setting the log option syslog-address similar to this: --log-opt syslog-address=udp://192.168.42.42:123.

#API running on docker like statistics in pretty-print format
$ curl --unix-socket /var/run/docker.sock http://v1/containers/91c86ec7b33f/stats | head -1 | python3 json.tool

#health check git location  https://github.com/spkane/rocketchat-hubot-demo.git

#docker events to monitor events on container
$ docker events
#or the equivalent
$ curl --unix-socket /var/run/docker.sock http://v1/events

#cadivisor to have charts on 8080 port of the docker container which run cadvisor or rest on 8080/api/v1.3/containers/
$ docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor google/cadvisor:latest
#for fedora or RHEL you need also the volume :  --volume=/cgroup:/cgroup \.

#In Dockerfile the COPY will copy the host file into container the ADD will get from network or untar the tar
#on ADD you could said what type of security --chown=UID:GID
#WORKDIR change the context of layers is not equivalent with RUN cd because the last one is not persisted
#Put on the first part of the Dockerfile the part that is not change every time so those layers will not be rebuild each time.
#use .dockerignore which is like .gitignore to filter the files which will be put pattern to exclude files from the directory.
#docker image save and load should be use to create a tarball from an image and recreate the image from the tarball.

#Image name: <registry URL>/<User or Org>/<name>:<tag>
#<registry URL> the default is docker.io but thare are more like :
# https://cloud.google.com/container-registry
# https://aws.amazon.com/ecr/
# https://azure.microsoft.com/en-us/services/container-registry/
# https://access.redhat.com/container/
# https://jfrog.com/integration/artifactory-docker-registry

# docker inspect could use --format to filter the result

########################NETWORK ###############################################
# Bridge is provided by docker and is local. Simple network based on Linux bridgs allowing networking on a single host
# Macvlan is provided by docker and is local. Configures multiple layer 2 (MAC) addresses on a single physical host interface. When migrating legacy system to a container-native one, this can be a really useful step.
# Overlay is provided by docker and is global. Multinode-capable container network based on Virtual Extensible LAN (VXLan) is used by docker swarm
# Weave Net is provided by Weaveworks and is global. Simple, resilient, multihost Docker networking.
# Contiv Network Plugin is provided by Cisco and is global . Open Source container networking.

#To list network on host use:
$ docker network ls

#To specify the network range (subnet) when creating a new network you should use like this:
$ docker network create --driver bridge --subnet "10.1.0.0/16" test-net

#Host network should never be used in production is the same network for the container as the host (like the same ip)
#you could ask the running container to run in the same network as another container specifing  --network container:<containerName>
#to expose port use -p <host port>:<container port>/<protocol like tcp/udp> the tcp is the default protocol
$ docker network inspect <network_id>

############################ DOCKER COMPOSE #####################################################

# githubs links: https://github.com/avescodes/docker-compose-in-depth

#volumes: declare which volume to be created at first run, this will be not recreated after first run
#volumes inside service declaration could be a volume declared or a directory from host as is mount using -v
#by default docker-compose up is using interactive mode so the terminal si blocked to run as daemon use -d
#docker compose prefixed all containers with the name of directory in which reside docker-compose.yml
#you could scale the application at start usingd --scale <serviceName>=<nr of instances>
#you could scale later on using docker-compose sclale <serviceName>=<nr of instances>
#build option in yml is the directory (context) where it should find the Dockerfile it could be specified by: context: <directory where is the dockerfile> and dockerfile: <the name of the dockerfile if is not Dockerfile>
#To push docker compose use:
$ docker-compose -f Dockerfile push

#useful links : https://docs.docker.com/compose/   https://docs.docker.com/compose/compose-file/
#link use <service>:<name> will deliver the property name to the container
# docker-compose rm #remove data from a containers
# docker-compose network default is bridge which link all containers
$ docker-compose exec <service_name> sh
# container_name is global even to docker host machine and could not scale using docker-compose scale
# links should be use in the same network it was working before in version 1
# when contianer is replaced the old connection between the remaining container and the old remain open but no data is transfer but a new connection will work !!
# use external_links to make an internal name to an external container which is connected to an external network, both should be share the network (external network should be defined in networks as external: true)
# use docker-compose port to find out which port is mapped to docker host
$ docker-compose port <service_name> <port_number>
# use external for external volume also
# see docs.docker.com/engine/extend/plugins/ to see the supported volume plugins
# docs.docker.com/engine/admin/logging/overview
# to get logs from the containers the default driver is json-file
$ docker-compose logs <container_id>
# paptertrailapp.com for free logging service to be used: driver is syslog and options is syslog-address: "udp://logs4.papertrailapp.com:33624"
# when you use external system docmer-compose logs is not available
########## docker-compose CLI ###############
# the dirrectory is used as prefix for the container names
# COMPOSE_NAME_PROJECT or --project-name when start an evironment instead of directory
# doenv from github is a good project to mangage development environments
# docker-composer events give events from the docker compose
# docker-compose config check the config file and reports errors if it has also if is correct it will print it
# orphan containers are containers that have been run using docker-compose run command
# in docker-compose.yml it is possible to run with ${}
# .env is read the by the docker-compose and replace the environments
# each service could have a key env_file: with a list of files.
# on evironment key if you specify variable without value it will be taken from the external environment
# configuration files are loaded left to right
# docker-compose.override.yml will override configuration from the docker-compose file but only if you don't specify file with -f


############################## DOCKER SECURITY ####################################################

#run the container in privileged mode so it you want to add device mount etc : add at container start --privileged=true
#The container could be run with non privileged user using -u <userId> otherwise is root
#all containers could be run with non priviledged user using --serns-remap in dockerd command
#if in priviledged container you mount swap using swapon <swap file> be sure you do swapoff before exit the container otherwise the container couln't be destroy
#if you forget that you could unmount from host using root like:
$ sudo swapoff /var/lib/docker/overlay/<containerId>/upper/<swap_file>
#if you want only some priviledged use --cap-add=<priviledge> and if you want to disable some priviledge you use --cap-drop=<priviledge>
#you could use  seccomp profile file to fine grain the capability of the container
#use -tlsverify to require certificates if you are in production

############################### DOCKER ARCHITECTURE #################################################

# dockerd - one per server, build container images, management : high level network, volumes, logging, statistics etc
# containerd - one per server, manages lifecycle execution, copy on write fileisystem an low level network
# docker-containerd-shim - one per container, handle IO and reports exit status
# runc - constructs the container and execution, statistics, reports events on lifecycle
#runtime continer could be Clear Containers (cc-runtime), runc or Railcar to select one use
$ docker run --runtime <runtime> -d <command>
#to see which container works with by example use :
$ sudo cc-runtime list
#gVisor is for high secure isolation and for for massive scalling, the runtime is runsc

############################### OTHER STUFF #######################################################

#If you use a process manager or initialization system on your servers, like upstart or systemd, it is usually very easy to direct all process output to STDOUT and STDERR and then have your process monitor capture them and send them to a remote logging host.
# progrim for loadbalancer
# infrastructure teraform

############################### Orchestrators #####################################################
############################### Docker Swarm ######################################################
############################### Deprecated and removed ############################################
#Only one manager node for dev, test and demo, three manager in small to medium size swarm and five managers for large to extra large swarms
#a worker communicate only with three neighbors
#init the swarm using
$ docker swarm init
#join to the swarm
$ docker swarm join --token <join-token> <IP address>:2377
#where <join-token> is a token generated by the swarm leader and <IP address> is the address of the leader
#to list the nodes into the swarm use
$ docker node ls
#then using inspect find the ip

#create the default machine for
$ docker-machine create --driver virtualbox default
#start the default machine
$ docker-machine start default

#create the swarm
# get IP of Swarm leader
$ export IP=$(docker-machine ip node-1)
# init the Swarm
$ docker-machine ssh node-1 docker swarm init --advertise-addr $IP
# Get the Swarm join-token
$ export JOIN_TOKEN=$(docker-machine ssh node-1 docker swarm join-token worker -q)\
#join the swarm
$ docker-machine ssh $NODE_NAME docker swarm join --token $JOIN_TOKEN $IP:2377
#to promote a leader use docker node promote <nameOfNode>

#script to create auto
alias dm="docker-machine"
for NODE in `seq 1 5`; do
  NODE_NAME=node-${NODE}
  dm rm --force $NODE_NAME
  dm create --driver virtualbox $NODE_NAME
done
alias dms="docker-machine ssh"
export IP=$(docker-machine ip node-1)
dms node-1 docker swarm init --advertise-addr $IP;
export JOIN_TOKEN=$(dms node-1 docker swarm join-token worker -q);
for NODE in `seq 2 5`; do
  NODE_NAME="node-${NODE}"
  dms $NODE_NAME docker swarm join --token $JOIN_TOKEN $IP:2377
done;
dms node-1 docker node promote node-2 node-3

#to deploy a stack configuration to the docker swarm use:
$ docker stack deploy -c <stack.yaml> <nameOfStack>
#to list the stacks use:
$ docker stack ls
#to list services use:
$ docker service ls

#in yml the update_config: is used for rolling update using parallelism mean batches and delay between batches failure_action: <rollback> is used to know what to do if it fail, monitor: <time> is used to define how long newly deployed task should be monitored for health as a decision point whether or not to continue with the next batch
#in yml the expose port is <hostPort>:<containerPort>
#all logs are agregated into the call docker service logs without containter id. if you want logs from a specific container  add the task id which could be obtained using docker service ps
#if a node is stopped and then it return to started he will not get back the old tasks but it will be ready for a new workload

#healthcheck: has test:[....] interval: timeout: retries: start_period:
#Docker swarm could not be used to have blue-green deploy strategy

#Secrets created from stdin:
$ echo "sample secret value" | docker secret create sample-secret - 
#Secrets created from file:
$ docker secret create other-secret <filePath>

#Secrets are used by services and usually they are asigned to a service at creation like
$ docker service create --name web --secret api-secret-key --publish 8000:8000 fundamentalsofdocker/whoami:latest
#The secret will be in running container at /run/secrets/<secret-key>
#it could be customized like :
$ docker service create --name web -p 8000:8000 --secret source=api-secret-key,target=/run/my-secrets/api-secret-key fundamentalsofdocker/whoami:latest

#to simulate secrets in dev environment:
$ docker container run -d --name whoami -p 8000:8000 -v $(pwd)/dev-secrets:/run/secrets fundamentalsofdocker/whoami:latest

#for legacy application which use yaml 
file=/app/bin/app.conf
demo_secret=`cat /run/secret/demo-secret`
sed -i "s/<<demo-secret-value>>/$demo_secret/g" "$file"

#To update secrets into a running service first remove it
$ docker service update --secret-rm db-password web
#then add the update secret
$ docker service update --secret-add source=db-password-v2, target=db-password web


################################## KUBERNETES ##############################################
#links
https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/
https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux
https://kubernetes.io/docs/tasks/tools/install-minikube/

#install kubectl
$ curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

#or 
$ sudo apt-get update && sudo apt-get install -y apt-transport-https
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
$ sudo apt-get update
$ sudo apt-get install -y kubectl

#install minikube
$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
$ chmod +x minukube
$ sudo install minikube /usr/local/bin/
#start the minikube
$ minikube start --vm-driver=<driver_name>
$ minikube status
$ kubectl version
$ kubectl config current-context
$ kubectl get-contexts
$ kubectl config use-context minikube
$ kubectl get nodes
$ kubectl cluster-info

#update bash completion
$ echo 'source <(kubectl completion bash)' >>~/.bashrc
$ kubectl completion bash > kubectl
$ sudo mv kubectl /etc/bash_completion.d/

#it is ok to have workers in hybrid OS : linux an Windows Server 2010 from kubernetes 1.10

#useful commands

https://kubernetes.io/docs/reference/kubectl/cheatsheet/

#MINIKUBE does not have LoadBallancer integrated so you should use an external one or emulate but is not working ...
$ minikube tunnel

#To describe a pod use :
$ kubectl describe pod/<name Of Pod>

#To see the persisted volume claim use
$ kubectl get pvc

#To enter into a pod use:
$ kubectl exec -it <NameOfPod> -- /bin/sh

#example:
$ kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
$ kubectl expose deployment hello-minikube --type=NodePort --port=8080
$ minikube service hello-minikube

##
$ kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080 deployment "hello-minikube" created
$ kubectl expose deployment hello-minikube --type=NodePort service "hello-minikube" exposed
$ kubectl service hello-minikube --url

#full scale deployment example
$ git clone https://github.com/bluewhalebook/docker-up-and-running-2nd-edition.git

$ http://192.168.99.110:30171/documents/docker-up-and-running-public/sample.pdf?page=1

#bubernetes and load balancer

https://blog.codonomics.com/2019/02/loadbalancer-support-with-minikube-for-k8s.html

https://metallb.universe.tf/installation/network-addons/

https://stackoverflow.com/questions/44110876/kubernetes-service-external-ip-pending

#ClusterIp is used for services that are not supposed to be seen outside of cluster in contrast with NodePort and LoadBalancer
#StatefulSet is used for db and persistence in contrast with ReplicaSet

#kubectl edit <service> could be used to modify running service for blue-green roll

#Secret created manually using base64 with kind Secret and type Opaque and data with secret could be retrieved back using kubectl get secrets/<secretName> -o yaml
#it should be created using kubectl command like
$ kubectl create secret generic pets-secret-prod --from-file=./username.txt --from-file=./password.txt
#to be used use in yaml the:
#volumeMounts:
# - name: secrets
#   mountPath: "/etc/secrets"
#   readOnly: true
#volumes:
# - name: secrets
#   secret:
#      secretName: pets-secret
#to be available in env use:
# env:
# - name: <PETS_USERNAME>
#   valueFrom:
#	secretKeyRef:
#		name:
#		key

##dont forget to delete the deployment then everyting is deleted otherwise the replicaset will be created each time


